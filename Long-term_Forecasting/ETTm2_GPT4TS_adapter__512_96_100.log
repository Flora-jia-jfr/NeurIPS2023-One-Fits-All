train 237671
val 79975
test 79975
gpt2 = GPT2AdapterModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-5): 6 x GPT2BlockWithAdapters(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2AttentionWithAdapters(
          (c_attn): MergedLinear(
            in_features=768, out_features=2304, bias=True
            (loras): ModuleDict()
          )
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
          (prefix_tuning): PrefixTuningLayer(
            (prefix_gates): ModuleDict()
            (pool): PrefixTuningPool(
              (prefix_tunings): ModuleDict()
            )
          )
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Linear(
            in_features=768, out_features=3072, bias=True
            (loras): ModuleDict()
          )
          (c_proj): Linear(
            in_features=3072, out_features=768, bias=True
            (loras): ModuleDict()
          )
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (attention_adapters): BottleneckLayer(
          (adapters): ModuleDict(
            (ts_adapter): Adapter(
              (non_linearity): Activation_Function_Class(
                (f): SiLUActivation()
              )
              (adapter_down): Sequential(
                (0): Linear(in_features=768, out_features=48, bias=True)
                (1): Activation_Function_Class(
                  (f): SiLUActivation()
                )
              )
              (adapter_up): Linear(in_features=48, out_features=768, bias=True)
            )
          )
          (adapter_fusion_layer): ModuleDict()
        )
        (output_adapters): BottleneckLayer(
          (adapters): ModuleDict(
            (ts_adapter): Adapter(
              (non_linearity): Activation_Function_Class(
                (f): SiLUActivation()
              )
              (adapter_down): Sequential(
                (0): Linear(in_features=768, out_features=48, bias=True)
                (1): Activation_Function_Class(
                  (f): SiLUActivation()
                )
              )
              (adapter_up): Linear(in_features=48, out_features=768, bias=True)
            )
          )
          (adapter_fusion_layer): ModuleDict()
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (invertible_adapters): ModuleDict()
    (shared_parameters): ModuleDict()
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (default): CausalLMHead(
      (0): Linear(in_features=768, out_features=50257, bias=False)
    )
  )
)
transformer.wpe.weight
transformer.h.0.ln_1.weight
transformer.h.0.ln_1.bias
transformer.h.0.ln_2.weight
transformer.h.0.ln_2.bias
transformer.h.0.attention_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.0.attention_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.0.attention_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.0.attention_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.0.output_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.0.output_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.0.output_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.0.output_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.1.ln_1.weight
transformer.h.1.ln_1.bias
transformer.h.1.ln_2.weight
transformer.h.1.ln_2.bias
transformer.h.1.attention_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.1.attention_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.1.attention_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.1.attention_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.1.output_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.1.output_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.1.output_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.1.output_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.2.ln_1.weight
transformer.h.2.ln_1.bias
transformer.h.2.ln_2.weight
transformer.h.2.ln_2.bias
transformer.h.2.attention_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.2.attention_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.2.attention_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.2.attention_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.2.output_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.2.output_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.2.output_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.2.output_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.3.ln_1.weight
transformer.h.3.ln_1.bias
transformer.h.3.ln_2.weight
transformer.h.3.ln_2.bias
transformer.h.3.attention_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.3.attention_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.3.attention_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.3.attention_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.3.output_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.3.output_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.3.output_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.3.output_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.4.ln_1.weight
transformer.h.4.ln_1.bias
transformer.h.4.ln_2.weight
transformer.h.4.ln_2.bias
transformer.h.4.attention_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.4.attention_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.4.attention_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.4.attention_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.4.output_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.4.output_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.4.output_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.4.output_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.5.ln_1.weight
transformer.h.5.ln_1.bias
transformer.h.5.ln_2.weight
transformer.h.5.ln_2.bias
transformer.h.5.attention_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.5.attention_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.5.attention_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.5.attention_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.5.output_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.5.output_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.5.output_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.5.output_adapters.adapters.ts_adapter.adapter_up.bias
transformer.ln_f.weight
transformer.ln_f.bias
Epoch: 1 cost time: 292.08789801597595
Epoch: 1, Steps: 928 | Train Loss: 0.2403147 Vali Loss: 0.1316057
lr = 0.0019510568
Validation loss decreased (inf --> 0.131606).  Saving model ...
Epoch: 2 cost time: 299.0043234825134
Epoch: 2, Steps: 928 | Train Loss: 0.1671892 Vali Loss: 0.1280622
lr = 0.0018090179
Validation loss decreased (0.131606 --> 0.128062).  Saving model ...
Epoch: 3 cost time: 297.1332678794861
Epoch: 3, Steps: 928 | Train Loss: 0.1445435 Vali Loss: 0.1318348
lr = 0.0015877873
EarlyStopping counter: 1 out of 3
Epoch: 4 cost time: 297.16947078704834
Epoch: 4, Steps: 928 | Train Loss: 0.1321478 Vali Loss: 0.1306658
lr = 0.0013090204
EarlyStopping counter: 2 out of 3
Epoch: 5 cost time: 296.69276189804077
Epoch: 5, Steps: 928 | Train Loss: 0.1207500 Vali Loss: 0.1376989
lr = 0.0010000050
EarlyStopping counter: 3 out of 3
Early stopping
------------------------------------
test shape: (312, 256, 96, 1) (312, 256, 96, 1)
test shape: (79872, 96, 1) (79872, 96, 1)
mae:0.2681, mse:0.1864, rmse:0.4317, smape:44.7824
train 237671
val 79975
test 79975
gpt2 = GPT2AdapterModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-5): 6 x GPT2BlockWithAdapters(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2AttentionWithAdapters(
          (c_attn): MergedLinear(
            in_features=768, out_features=2304, bias=True
            (loras): ModuleDict()
          )
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
          (prefix_tuning): PrefixTuningLayer(
            (prefix_gates): ModuleDict()
            (pool): PrefixTuningPool(
              (prefix_tunings): ModuleDict()
            )
          )
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Linear(
            in_features=768, out_features=3072, bias=True
            (loras): ModuleDict()
          )
          (c_proj): Linear(
            in_features=3072, out_features=768, bias=True
            (loras): ModuleDict()
          )
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (attention_adapters): BottleneckLayer(
          (adapters): ModuleDict(
            (ts_adapter): Adapter(
              (non_linearity): Activation_Function_Class(
                (f): SiLUActivation()
              )
              (adapter_down): Sequential(
                (0): Linear(in_features=768, out_features=48, bias=True)
                (1): Activation_Function_Class(
                  (f): SiLUActivation()
                )
              )
              (adapter_up): Linear(in_features=48, out_features=768, bias=True)
            )
          )
          (adapter_fusion_layer): ModuleDict()
        )
        (output_adapters): BottleneckLayer(
          (adapters): ModuleDict(
            (ts_adapter): Adapter(
              (non_linearity): Activation_Function_Class(
                (f): SiLUActivation()
              )
              (adapter_down): Sequential(
                (0): Linear(in_features=768, out_features=48, bias=True)
                (1): Activation_Function_Class(
                  (f): SiLUActivation()
                )
              )
              (adapter_up): Linear(in_features=48, out_features=768, bias=True)
            )
          )
          (adapter_fusion_layer): ModuleDict()
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (invertible_adapters): ModuleDict()
    (shared_parameters): ModuleDict()
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (default): CausalLMHead(
      (0): Linear(in_features=768, out_features=50257, bias=False)
    )
  )
)
transformer.wpe.weight
transformer.h.0.ln_1.weight
transformer.h.0.ln_1.bias
transformer.h.0.ln_2.weight
transformer.h.0.ln_2.bias
transformer.h.0.attention_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.0.attention_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.0.attention_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.0.attention_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.0.output_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.0.output_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.0.output_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.0.output_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.1.ln_1.weight
transformer.h.1.ln_1.bias
transformer.h.1.ln_2.weight
transformer.h.1.ln_2.bias
transformer.h.1.attention_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.1.attention_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.1.attention_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.1.attention_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.1.output_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.1.output_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.1.output_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.1.output_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.2.ln_1.weight
transformer.h.2.ln_1.bias
transformer.h.2.ln_2.weight
transformer.h.2.ln_2.bias
transformer.h.2.attention_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.2.attention_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.2.attention_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.2.attention_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.2.output_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.2.output_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.2.output_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.2.output_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.3.ln_1.weight
transformer.h.3.ln_1.bias
transformer.h.3.ln_2.weight
transformer.h.3.ln_2.bias
transformer.h.3.attention_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.3.attention_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.3.attention_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.3.attention_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.3.output_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.3.output_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.3.output_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.3.output_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.4.ln_1.weight
transformer.h.4.ln_1.bias
transformer.h.4.ln_2.weight
transformer.h.4.ln_2.bias
transformer.h.4.attention_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.4.attention_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.4.attention_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.4.attention_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.4.output_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.4.output_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.4.output_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.4.output_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.5.ln_1.weight
transformer.h.5.ln_1.bias
transformer.h.5.ln_2.weight
transformer.h.5.ln_2.bias
transformer.h.5.attention_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.5.attention_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.5.attention_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.5.attention_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.5.output_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.5.output_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.5.output_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.5.output_adapters.adapters.ts_adapter.adapter_up.bias
transformer.ln_f.weight
transformer.ln_f.bias
Epoch: 1 cost time: 297.1104483604431
Epoch: 1, Steps: 928 | Train Loss: 0.2525348 Vali Loss: 0.1251461
lr = 0.0019510568
Validation loss decreased (inf --> 0.125146).  Saving model ...
Epoch: 2 cost time: 297.4731297492981
Epoch: 2, Steps: 928 | Train Loss: 0.1762705 Vali Loss: 0.1260803
lr = 0.0018090179
EarlyStopping counter: 1 out of 3
Epoch: 3 cost time: 299.61398458480835
Epoch: 3, Steps: 928 | Train Loss: 0.1529110 Vali Loss: 0.1293318
lr = 0.0015877873
EarlyStopping counter: 2 out of 3
Epoch: 4 cost time: 307.5850341320038
Epoch: 4, Steps: 928 | Train Loss: 0.1381544 Vali Loss: 0.1394193
lr = 0.0013090204
EarlyStopping counter: 3 out of 3
Early stopping
------------------------------------
test shape: (312, 256, 96, 1) (312, 256, 96, 1)
test shape: (79872, 96, 1) (79872, 96, 1)
mae:0.2662, mse:0.1798, rmse:0.4241, smape:44.5736
train 237671
val 79975
test 79975
gpt2 = GPT2AdapterModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-5): 6 x GPT2BlockWithAdapters(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2AttentionWithAdapters(
          (c_attn): MergedLinear(
            in_features=768, out_features=2304, bias=True
            (loras): ModuleDict()
          )
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
          (prefix_tuning): PrefixTuningLayer(
            (prefix_gates): ModuleDict()
            (pool): PrefixTuningPool(
              (prefix_tunings): ModuleDict()
            )
          )
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Linear(
            in_features=768, out_features=3072, bias=True
            (loras): ModuleDict()
          )
          (c_proj): Linear(
            in_features=3072, out_features=768, bias=True
            (loras): ModuleDict()
          )
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (attention_adapters): BottleneckLayer(
          (adapters): ModuleDict(
            (ts_adapter): Adapter(
              (non_linearity): Activation_Function_Class(
                (f): SiLUActivation()
              )
              (adapter_down): Sequential(
                (0): Linear(in_features=768, out_features=48, bias=True)
                (1): Activation_Function_Class(
                  (f): SiLUActivation()
                )
              )
              (adapter_up): Linear(in_features=48, out_features=768, bias=True)
            )
          )
          (adapter_fusion_layer): ModuleDict()
        )
        (output_adapters): BottleneckLayer(
          (adapters): ModuleDict(
            (ts_adapter): Adapter(
              (non_linearity): Activation_Function_Class(
                (f): SiLUActivation()
              )
              (adapter_down): Sequential(
                (0): Linear(in_features=768, out_features=48, bias=True)
                (1): Activation_Function_Class(
                  (f): SiLUActivation()
                )
              )
              (adapter_up): Linear(in_features=48, out_features=768, bias=True)
            )
          )
          (adapter_fusion_layer): ModuleDict()
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (invertible_adapters): ModuleDict()
    (shared_parameters): ModuleDict()
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (default): CausalLMHead(
      (0): Linear(in_features=768, out_features=50257, bias=False)
    )
  )
)
transformer.wpe.weight
transformer.h.0.ln_1.weight
transformer.h.0.ln_1.bias
transformer.h.0.ln_2.weight
transformer.h.0.ln_2.bias
transformer.h.0.attention_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.0.attention_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.0.attention_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.0.attention_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.0.output_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.0.output_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.0.output_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.0.output_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.1.ln_1.weight
transformer.h.1.ln_1.bias
transformer.h.1.ln_2.weight
transformer.h.1.ln_2.bias
transformer.h.1.attention_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.1.attention_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.1.attention_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.1.attention_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.1.output_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.1.output_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.1.output_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.1.output_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.2.ln_1.weight
transformer.h.2.ln_1.bias
transformer.h.2.ln_2.weight
transformer.h.2.ln_2.bias
transformer.h.2.attention_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.2.attention_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.2.attention_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.2.attention_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.2.output_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.2.output_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.2.output_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.2.output_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.3.ln_1.weight
transformer.h.3.ln_1.bias
transformer.h.3.ln_2.weight
transformer.h.3.ln_2.bias
transformer.h.3.attention_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.3.attention_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.3.attention_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.3.attention_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.3.output_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.3.output_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.3.output_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.3.output_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.4.ln_1.weight
transformer.h.4.ln_1.bias
transformer.h.4.ln_2.weight
transformer.h.4.ln_2.bias
transformer.h.4.attention_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.4.attention_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.4.attention_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.4.attention_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.4.output_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.4.output_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.4.output_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.4.output_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.5.ln_1.weight
transformer.h.5.ln_1.bias
transformer.h.5.ln_2.weight
transformer.h.5.ln_2.bias
transformer.h.5.attention_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.5.attention_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.5.attention_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.5.attention_adapters.adapters.ts_adapter.adapter_up.bias
transformer.h.5.output_adapters.adapters.ts_adapter.adapter_down.0.weight
transformer.h.5.output_adapters.adapters.ts_adapter.adapter_down.0.bias
transformer.h.5.output_adapters.adapters.ts_adapter.adapter_up.weight
transformer.h.5.output_adapters.adapters.ts_adapter.adapter_up.bias
transformer.ln_f.weight
transformer.ln_f.bias
Epoch: 1 cost time: 309.83929419517517
Epoch: 1, Steps: 928 | Train Loss: 0.2290352 Vali Loss: 0.1217956
lr = 0.0019510568
Validation loss decreased (inf --> 0.121796).  Saving model ...
Epoch: 2 cost time: 311.00668263435364
Epoch: 2, Steps: 928 | Train Loss: 0.1711436 Vali Loss: 0.1257188
lr = 0.0018090179
EarlyStopping counter: 1 out of 3
Epoch: 3 cost time: 312.2554974555969
Epoch: 3, Steps: 928 | Train Loss: 0.1514199 Vali Loss: 0.1295835
lr = 0.0015877873
EarlyStopping counter: 2 out of 3
Epoch: 4 cost time: 311.7014615535736
Epoch: 4, Steps: 928 | Train Loss: 0.1437437 Vali Loss: 0.1271702
lr = 0.0013090204
EarlyStopping counter: 3 out of 3
Early stopping
------------------------------------
test shape: (312, 256, 96, 1) (312, 256, 96, 1)
test shape: (79872, 96, 1) (79872, 96, 1)
mae:0.2609, mse:0.1701, rmse:0.4124, smape:44.1940
mse_mean = 0.1788, mse_std = 0.0067
mae_mean = 0.2651, mae_std = 0.0031
